{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Covid-19 Project",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMp014tPvhibIsHiJnu50zw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jliu1224/PythonProject/blob/main/Twitter_Sentiment_Covid_19_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxDCrGi_K6Os",
        "outputId": "b233ce2f-2348-4762-d871-bf7948f4e300"
      },
      "source": [
        "from google.colab import files, drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RllyJTJW7rzx"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import re\n",
        "import string as st\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-n_uFv1MhSx"
      },
      "source": [
        "## **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "CKS7VVFWETU3",
        "outputId": "d4380241-19a1-407b-e7bd-22549c7a509d"
      },
      "source": [
        "df = pd.read_csv('CoronaTwitters.csv', encoding = \"ISO-8859-1\", engine='python')\n",
        "df.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserName</th>\n",
              "      <th>ScreenName</th>\n",
              "      <th>Location</th>\n",
              "      <th>TweetAt</th>\n",
              "      <th>OriginalTweet</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>44953</td>\n",
              "      <td>NYC</td>\n",
              "      <td>2/3/2020</td>\n",
              "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
              "      <td>Extremely Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>44954</td>\n",
              "      <td>Seattle, WA</td>\n",
              "      <td>2/3/2020</td>\n",
              "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>44955</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2/3/2020</td>\n",
              "      <td>Find out how you can protect yourself and love...</td>\n",
              "      <td>Extremely Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>44956</td>\n",
              "      <td>Chicagoland</td>\n",
              "      <td>2/3/2020</td>\n",
              "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>44957</td>\n",
              "      <td>Melbourne, Victoria</td>\n",
              "      <td>3/3/2020</td>\n",
              "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserName  ...           Sentiment\n",
              "0         1  ...  Extremely Negative\n",
              "1         2  ...            Positive\n",
              "2         3  ...  Extremely Positive\n",
              "3         4  ...            Negative\n",
              "4         5  ...             Neutral\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "iB78YZgPLkYp",
        "outputId": "519661d3-08b0-4654-91f8-9f3b72568977"
      },
      "source": [
        "#Read Data and create labels\n",
        "texts = df[['OriginalTweet', 'Sentiment']]\n",
        "sentiments = {'Extremely Positive':2, 'Positive':2, 'Neutral':1, 'Negative':0, 'Extremely Negative':0}\n",
        "texts['labels'] = texts['Sentiment'].str.strip().map(sentiments)\n",
        "texts.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OriginalTweet</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
              "      <td>Extremely Negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Find out how you can protect yourself and love...</td>\n",
              "      <td>Extremely Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       OriginalTweet  ... labels\n",
              "0  TRENDING: New Yorkers encounter empty supermar...  ...      0\n",
              "1  When I couldn't find hand sanitizer at Fred Me...  ...      2\n",
              "2  Find out how you can protect yourself and love...  ...      2\n",
              "3  #Panic buying hits #NewYork City as anxious sh...  ...      0\n",
              "4  #toiletpaper #dunnypaper #coronavirus #coronav...  ...      1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGi_yumvmokd",
        "outputId": "84a4dbc1-a0a7-44a8-b482-cc537784ba27"
      },
      "source": [
        "#Clean the data in Original Tweet\r\n",
        "tweets = texts['OriginalTweet']\r\n",
        "lowercase = tweets.map(lambda t : t.lower())\r\n",
        "noa = lowercase.str.replace('â', '', n=-1)\r\n",
        "nohttp = noa.str.replace('[http://][t.co][/][/D+]', '', n=-1)\r\n",
        "nopunc = nohttp.str.replace('[,().?!-\":#]', '', n=-1)\r\n",
        "noat = nopunc.str.replace('[@]\\w+', '', n=-1)\r\n",
        "non = noat.str.replace('\\n', '', n=-1)\r\n",
        "new_tweets = non.str.replace('/', ' ', n=-1)\r\n",
        "new_tweets"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        trending new yorkers encounter empty supermark...\n",
              "1        when i couldn't find hand sanitizer at fred me...\n",
              "2        find out how you can protect yourself and love...\n",
              "3        panic buying hits newyork city as anxious shop...\n",
              "4        toiletpaper dunnypaper coronavirus coronavirus...\n",
              "                               ...                        \n",
              "44950    airline pilots offering to stock supermarket s...\n",
              "44951    response to complaint not provided citing covi...\n",
              "44952    you know its getting tough when   is rationin...\n",
              "44953    is it wrong that the smell of hand sanitizer i...\n",
              "44954     well new used rift s are going for $70000 on ...\n",
              "Name: OriginalTweet, Length: 44955, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP_-IGqJQyN2",
        "outputId": "13189a8f-73a9-4896-c3aa-6d6ba0f97b77"
      },
      "source": [
        "texts['OriginalTweet'] = new_tweets"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aftNNOCGVkcO",
        "outputId": "a32be90d-41e3-46dd-988a-5f4d6d009257"
      },
      "source": [
        "#Get the data\n",
        "twitters = new_tweets.values\n",
        "labels = texts.labels.values\n",
        "#Using TF-IDF to represent each twitter record, denoted as X;\n",
        "#Create a label vector Y\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=500, ngram_range=(1,1)) \n",
        "instances = vectorizer.fit_transform(twitters)\n",
        "X = instances.toarray()\n",
        "Y = labels\n",
        "\n",
        "print('The shape of X is:', X.shape)\n",
        "print('The shape of Y is:', Y.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of X is: (44955, 500)\n",
            "The shape of Y is: (44955,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiTybuI1M831"
      },
      "source": [
        "# **Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmIS--fhVyBB",
        "outputId": "6f13bc90-dcc2-4161-8533-ae6dec3ff973"
      },
      "source": [
        "#Building a random forest classifier as one of the baselines\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,\r\n",
        "                                                   random_state = 1997)\r\n",
        "rf_model = RandomForestClassifier(criterion='entropy', max_depth=9, random_state=1997, n_estimators=20)\r\n",
        "rf_model.fit(X_train, y_train)\r\n",
        "\r\n",
        "print('Random Forest - Accuracy on training set: {:.4f}'.format(rf_model.score(X_train, y_train)))\r\n",
        "print('Random Forest - Accuracy on test set: {:.4f}'.format(rf_model.score(X_test, y_test)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest - Accuracy on training set: 0.5679\n",
            "Random Forest - Accuracy on test set: 0.5712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6gm_h9RNKWz"
      },
      "source": [
        "## **Fully Connected Feedforward Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haASSGyi-tGP"
      },
      "source": [
        "#The parameters for the fully connected feedforward network\n",
        "epochs = 5\n",
        "lr = 2e-3\n",
        "indim = X.shape[1]\n",
        "outdim = 3\n",
        "drate = 0.8\n",
        "batch_size = 500\n",
        "\n",
        "#Create the dataset\n",
        "X_tensor = torch.from_numpy(X)\n",
        "Y_tensor = torch.from_numpy(Y)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, Y_tensor)\n",
        "train_size = int(0.8*len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(150))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiLIkpJMRrdZ",
        "outputId": "aebb2674-1643-4c2a-8b9f-881f7905446b"
      },
      "source": [
        "#Create the fully connected feedforward network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SentimentNetwork(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, dropout_rate):\n",
        "    \n",
        "    super(SentimentNetwork,self).__init__()\n",
        "    \n",
        "    self.fc1 = nn.Linear(input_dim, 375)\n",
        "    self.fc2 = nn.Linear(375, 244)\n",
        "    self.fc3 = nn.Linear(244, 100)\n",
        "    self.fc4 = nn.Linear(100, 12)\n",
        "    self.fc5 = nn.Linear(12, output_dim)\n",
        "    self.dropout = nn.Dropout(p=drate)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.dropout(F.relu(self.fc1(x))) \n",
        "    x = self.dropout(F.relu(self.fc2(x)))\n",
        "    x = self.dropout(F.relu(self.fc3(x)))\n",
        "    x = self.dropout(F.relu(self.fc4(x)))\n",
        "    x = F.softmax(self.fc5(x))\n",
        "   \n",
        "    return x\n",
        "\n",
        "# create a model\n",
        "model = SentimentNetwork(indim, outdim, drate)\n",
        "print(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentNetwork(\n",
            "  (fc1): Linear(in_features=500, out_features=375, bias=True)\n",
            "  (fc2): Linear(in_features=375, out_features=244, bias=True)\n",
            "  (fc3): Linear(in_features=244, out_features=100, bias=True)\n",
            "  (fc4): Linear(in_features=100, out_features=12, bias=True)\n",
            "  (fc5): Linear(in_features=12, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.8, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hzSOjpYYaGK"
      },
      "source": [
        "#Training function for one epoch\n",
        "def train(model, train_loader, optimizer, criterion):\n",
        "  \n",
        "  epoch_loss_total, epoch_acc_total = 0.0,0.0 # the loss and accuracy for each epoch\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):  \n",
        "    #Zero gradient\n",
        "    optimizer.zero_grad() \n",
        "    #predictions = calculate the predicted output for the current batch \n",
        "    predictions = model(data.float()) \n",
        "    #loss = calculate the loss for the current batch using predictions and the truth\n",
        "    loss = criterion(predictions, target) \n",
        "    #acc = calculate the accuracy using the predictions and the truth\n",
        "    pred = predictions.data.max(1)[1] # get the index of the max log-probability\n",
        "    acc = pred.eq(target.data).sum()\n",
        "    \n",
        "    #backpropagate\n",
        "    loss.backward() \n",
        "    optimizer.step() \n",
        "     \n",
        "    epoch_loss_total += loss.item()\n",
        "    epoch_acc_total += acc\n",
        "\n",
        "  #calculate the average epoch_loss and epoch_acc\n",
        "  epoch_loss = epoch_loss_total/len(train_loader.dataset)\n",
        "  epoch_acc = epoch_acc_total/len(train_loader.dataset)\n",
        "\n",
        "  return epoch_loss, epoch_acc\n",
        "\n",
        "#Validation process function for one epoch\n",
        "def evaluate(model, val_loader, criterion):\n",
        "  \n",
        "  epoch_loss_total, epoch_acc_total = 0.0,0.0 # the loss and accuracy for each epoch\n",
        "\n",
        "  model.eval()\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    for data, target in val_loader: \n",
        "      #predictions\n",
        "      predictions = model(data.float())\n",
        "      #loss\n",
        "      loss = criterion(predictions, target) \n",
        "      #acc\n",
        "      pred = predictions.data.max(1)[1]\n",
        "      acc = pred.eq(target.data).sum()\n",
        "\n",
        "      epoch_loss_total += loss.item()\n",
        "      epoch_acc_total += acc\n",
        "    #calculate the average epoch_loss and epoch_acc\n",
        "    epoch_loss = epoch_loss_total/len(val_loader.dataset)\n",
        "    epoch_acc = epoch_acc_total/len(val_loader.dataset)   \n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDixsWxOakDt",
        "outputId": "f5049b4a-71df-4a25-dfd0-8adb1787ef6d"
      },
      "source": [
        "#Performance of Fully Connected Feedforward Network\n",
        "#Define the optimizer and the learning rate\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "#Genearte a report on the performance\n",
        "for epoch in range(epochs):\n",
        "  train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
        "    \n",
        "  print(f'Epoch: {epoch+1:02}')\n",
        "  print(f'Train Loss: {train_loss:.4f}   Train Acc: {train_acc:.4f}')\n",
        "  print(f' Val. Loss: {valid_loss:.4f}    Val. Acc: {valid_acc:.4f}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "Train Loss: 0.0022   Train Acc: 0.3385\n",
            " Val. Loss: 0.0021    Val. Acc: 0.4410\n",
            "Epoch: 02\n",
            "Train Loss: 0.0021   Train Acc: 0.4434\n",
            " Val. Loss: 0.0020    Val. Acc: 0.5916\n",
            "Epoch: 03\n",
            "Train Loss: 0.0020   Train Acc: 0.5675\n",
            " Val. Loss: 0.0019    Val. Acc: 0.5997\n",
            "Epoch: 04\n",
            "Train Loss: 0.0019   Train Acc: 0.5890\n",
            " Val. Loss: 0.0019    Val. Acc: 0.6047\n",
            "Epoch: 05\n",
            "Train Loss: 0.0019   Train Acc: 0.5934\n",
            " Val. Loss: 0.0019    Val. Acc: 0.6069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1hSGq7VNS_L"
      },
      "source": [
        "## **Recurrent Neural Network (RNN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdRs9q5KNanK"
      },
      "source": [
        "### Data Preparation for RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnTOz7HoScZL",
        "outputId": "fbcbfd46-ea22-4daf-d08a-3b0c6f6de564"
      },
      "source": [
        "#Split the dataframe based on their labels\r\n",
        "positive_tweets = texts[texts['labels']==2]\r\n",
        "neutral_tweets = texts[texts['labels']==1]\r\n",
        "negative_tweets = texts[texts['labels']==0]\r\n",
        "\r\n",
        "#Sample from each tweet dataset based on a proportion of 19:17:8 (pos:neg:neu)\r\n",
        "#pos_samp = positive_tweets.sample(n=3800, random_state=1997)\r\n",
        "#neg_samp = negative_tweets.sample(n=3400, random_state=1997)\r\n",
        "#neu_samp = neutral_tweets.sample(n=1600, random_state=1997)\r\n",
        "\r\n",
        "#Construct a corpus and labels list\r\n",
        "corpus = []\r\n",
        "labels = []\r\n",
        "for doc in positive_tweets['OriginalTweet']:\r\n",
        "  corpus.append(doc.replace('\\n', ' '))\r\n",
        "  labels.append([1, 0, 0])\r\n",
        "for doc in neutral_tweets['OriginalTweet']:\r\n",
        "  corpus.append(doc.replace('\\n', ' '))\r\n",
        "  labels.append([0, 1, 0])\r\n",
        "for doc in negative_tweets['OriginalTweet']:\r\n",
        "  corpus.append(doc.replace('\\n', ' '))\r\n",
        "  labels.append([0, 0, 1])\r\n",
        "\r\n",
        "vectorizer_r = TfidfVectorizer(max_features=500, stop_words='english')\r\n",
        "X1 = vectorizer_r.fit_transform(corpus)\r\n",
        "y1 = np.array(labels)\r\n",
        "print(X1.shape, y1.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(44955, 500) (44955, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIfxa3-QVxEF",
        "outputId": "a8ba1fef-3c94-43ee-8a01-bc570caacb66"
      },
      "source": [
        "import torch\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "seq_length = -1\r\n",
        "\r\n",
        "word_tokenizer = vectorizer_r.build_tokenizer()\r\n",
        "vocab = vectorizer_r.vocabulary_\r\n",
        "\r\n",
        "doc_terms_list_train = [word_tokenizer(doc_str) for doc_str in corpus]\r\n",
        "docs = []\r\n",
        "for i in range(len(doc_terms_list_train)):\r\n",
        "  terms = []\r\n",
        "  for j in range(len(doc_terms_list_train[i])):\r\n",
        "    w = doc_terms_list_train[i][j]\r\n",
        "    if w in vocab:\r\n",
        "      terms.append(w)\r\n",
        "  if len(terms) > seq_length:\r\n",
        "    seq_length = len(terms)\r\n",
        "  docs.append(terms)\r\n",
        "\r\n",
        "max_features = 500\r\n",
        "datasets = np.zeros((X1.shape[0], seq_length, max_features))\r\n",
        "\r\n",
        "for i in range(len(docs)):\r\n",
        "  n_padding = seq_length - len(docs[i])\r\n",
        "\r\n",
        "  for j in range(len(docs[i])):\r\n",
        "    w = docs[i][j]\r\n",
        "    idx = vocab[w]\r\n",
        "    tfidf_val = X1[i, idx]\r\n",
        "    datasets[i, j+n_padding, idx] = tfidf_val\r\n",
        "\r\n",
        "datasets = datasets.astype(np.float32)\r\n",
        "y1 = y1.astype(np.float32)\r\n",
        "\r\n",
        "X1_train, X1_val, y1_train, y1_val = train_test_split(datasets, y1, test_size = 0.2, random_state = 1997)\r\n",
        "print(X1_train.shape, X1_val.shape, y1_train.shape, y1_val.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35964, 28, 500) (8991, 28, 500) (35964, 3) (8991, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyPnq25ZM_N3"
      },
      "source": [
        "#Same process with creating the datasets, but for RNN\r\n",
        "batch_size_r = 250\r\n",
        "\r\n",
        "train_data = TensorDataset(torch.from_numpy(X1_train), torch.from_numpy(y1_train))\r\n",
        "val_data = TensorDataset(torch.from_numpy(X1_val), torch.from_numpy(y1_val))\r\n",
        "\r\n",
        "train_loader_r = DataLoader(train_data, shuffle=True, batch_size=batch_size_r)\r\n",
        "val_loader_r = DataLoader(val_data, shuffle=True, batch_size=batch_size_r)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ze3BooNm4a"
      },
      "source": [
        "Building the RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNZbg5FamorQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7b4cb8-6e0d-41bf-84d7-148ec93dfd3e"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\r\n",
        "import torch\r\n",
        "import torch.nn as nn \r\n",
        "import torch.nn.functional as F \r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "#Parameters\r\n",
        "input_size = 500\r\n",
        "hidden_size = 400\r\n",
        "n_layers = 3\r\n",
        "output_size = 25\r\n",
        "\r\n",
        "class Model(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self, input_size, output_size, hidden_size, n_layers):\r\n",
        "    super().__init__()\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "    self.n_layers = n_layers\r\n",
        "    self.rnn = nn.RNN(input_size,hidden_size,n_layers,batch_first=True) # rnn layer\r\n",
        "    self.fc1 = nn.Linear(hidden_size,output_size) # rnn output (y_t) --> output (y'_t)\r\n",
        "    self.fc2 = nn.Linear(output_size,3) #the output from the last time period ->sentiment prediction\r\n",
        "\r\n",
        "  def forward(self,x, hidden):\r\n",
        "    batch_size = x.size()[0]\r\n",
        "    hidden = self.init_hidden(batch_size)\r\n",
        "    \r\n",
        "    rnn_out,hidden = self.rnn(x,hidden)\r\n",
        "    rnn_out = self.fc1(rnn_out)\r\n",
        "    last_out = rnn_out[:,-1,:].view(batch_size,-1)\r\n",
        "    out = F.softmax(self.fc2(last_out))\r\n",
        "    return out,hidden \r\n",
        "\r\n",
        "  def init_hidden(self,batch_size):\r\n",
        "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).cuda()\r\n",
        "    return hidden\r\n",
        "\r\n",
        "RNNmodel = Model(input_size, output_size, hidden_size, n_layers)\r\n",
        "\r\n",
        "print(RNNmodel)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (rnn): RNN(500, 400, num_layers=3, batch_first=True)\n",
            "  (fc1): Linear(in_features=400, out_features=25, bias=True)\n",
            "  (fc2): Linear(in_features=25, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWiEwqd-PC-o"
      },
      "source": [
        "Training and Validating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsJrSYW4VBgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce44b82-ecc4-42e1-f1fc-dc8cb6944800"
      },
      "source": [
        "#Train on GPU/CPU\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\r\n",
        "\r\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "if torch.cuda.is_available():\r\n",
        "  RNNmodel.to(device)\r\n",
        "\r\n",
        "#Define hyperparameters\r\n",
        "n_epochs = 6\r\n",
        "lr = 1e-4\r\n",
        "counter = 0\r\n",
        "clip = 5\r\n",
        "\r\n",
        "#Define loss and optimizer\r\n",
        "criterion_r = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(RNNmodel.parameters(), lr=lr)\r\n",
        "\r\n",
        "RNNmodel.train()\r\n",
        "\r\n",
        "for epochs in range(n_epochs):\r\n",
        "  #initiate hidden state\r\n",
        "  h = RNNmodel.init_hidden(batch_size_r)\r\n",
        "\r\n",
        "  #batch_loop\r\n",
        "  for inputs, labels in train_loader_r:\r\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "    counter += 1\r\n",
        "    \r\n",
        "    RNNmodel.zero_grad()\r\n",
        "\r\n",
        "    outputs, h = RNNmodel(inputs, h)\r\n",
        "\r\n",
        "    loss = criterion_r(outputs, torch.max(labels, 1)[1])\r\n",
        "    loss.backward()\r\n",
        "    pred = torch.max(outputs, 1)[1]\r\n",
        "    acc = pred.eq(torch.max(labels, 1)[1]).sum()\r\n",
        "\r\n",
        "    #Clip_grad_norm to help prevent the exploding gradient problem in RNNs\r\n",
        "    nn.utils.clip_grad_norm(RNNmodel.parameters(), clip)\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    ##Validation Loss\r\n",
        "    if counter % 10 == 0:\r\n",
        "      val_h = RNNmodel.init_hidden(batch_size_r)\r\n",
        "      val_losses = []\r\n",
        "\r\n",
        "\r\n",
        "      RNNmodel.eval()\r\n",
        "\r\n",
        "      for inputs, labels in val_loader_r:\r\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "        val_outputs, val_h = RNNmodel(inputs, val_h)\r\n",
        "        val_loss = criterion_r(val_outputs, torch.max(labels, 1)[1])\r\n",
        "        val_losses.append(val_loss.item())\r\n",
        "        pred = val_outputs.data.max(1)[1]\r\n",
        "        val_acc = pred.eq(labels.data.max(1)[1]).sum()\r\n",
        "\r\n",
        "      RNNmodel.train()\r\n",
        "\r\n",
        "      print('Epoch:{}/{}'.format(epochs+1, n_epochs),\r\n",
        "            'Batch:{}'.format(counter),\r\n",
        "            'Train Accuracy:{:.5f}'.format(acc/batch_size_r),\r\n",
        "            'Train Loss:{:.5f}'.format(loss.item()),\r\n",
        "            'Val Accuracy:{:.5f}'.format(val_acc/batch_size_r),\r\n",
        "            'Val Loss:{:.5f}'.format(np.mean(val_losses)))\r\n",
        "   "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1/6 Batch:10 Train Accuracy:0.45200 Train Loss:1.06112 Val Accuracy:0.42400 Val Loss:1.06455\n",
            "Epoch:1/6 Batch:20 Train Accuracy:0.49200 Train Loss:1.04621 Val Accuracy:0.44400 Val Loss:1.05393\n",
            "Epoch:1/6 Batch:30 Train Accuracy:0.46000 Train Loss:1.06052 Val Accuracy:0.44000 Val Loss:1.04995\n",
            "Epoch:1/6 Batch:40 Train Accuracy:0.47200 Train Loss:1.04488 Val Accuracy:0.39200 Val Loss:1.04979\n",
            "Epoch:1/6 Batch:50 Train Accuracy:0.44000 Train Loss:1.04446 Val Accuracy:0.43200 Val Loss:1.04959\n",
            "Epoch:1/6 Batch:60 Train Accuracy:0.41600 Train Loss:1.06291 Val Accuracy:0.41600 Val Loss:1.04961\n",
            "Epoch:1/6 Batch:70 Train Accuracy:0.38800 Train Loss:1.05370 Val Accuracy:0.44000 Val Loss:1.04954\n",
            "Epoch:1/6 Batch:80 Train Accuracy:0.42000 Train Loss:1.05903 Val Accuracy:0.42400 Val Loss:1.04971\n",
            "Epoch:1/6 Batch:90 Train Accuracy:0.43200 Train Loss:1.04375 Val Accuracy:0.40800 Val Loss:1.05086\n",
            "Epoch:1/6 Batch:100 Train Accuracy:0.38400 Train Loss:1.04519 Val Accuracy:0.41200 Val Loss:1.05107\n",
            "Epoch:1/6 Batch:110 Train Accuracy:0.46000 Train Loss:1.03263 Val Accuracy:0.38800 Val Loss:1.05016\n",
            "Epoch:1/6 Batch:120 Train Accuracy:0.44400 Train Loss:1.04196 Val Accuracy:0.37600 Val Loss:1.04942\n",
            "Epoch:1/6 Batch:130 Train Accuracy:0.44800 Train Loss:1.05064 Val Accuracy:0.40400 Val Loss:1.05175\n",
            "Epoch:1/6 Batch:140 Train Accuracy:0.42000 Train Loss:1.04771 Val Accuracy:0.37600 Val Loss:1.04945\n",
            "Epoch:2/6 Batch:150 Train Accuracy:0.40800 Train Loss:1.06111 Val Accuracy:0.42400 Val Loss:1.04968\n",
            "Epoch:2/6 Batch:160 Train Accuracy:0.38000 Train Loss:1.07829 Val Accuracy:0.43200 Val Loss:1.04932\n",
            "Epoch:2/6 Batch:170 Train Accuracy:0.41600 Train Loss:1.05646 Val Accuracy:0.42000 Val Loss:1.04962\n",
            "Epoch:2/6 Batch:180 Train Accuracy:0.44000 Train Loss:1.04424 Val Accuracy:0.39600 Val Loss:1.04932\n",
            "Epoch:2/6 Batch:190 Train Accuracy:0.47200 Train Loss:1.03823 Val Accuracy:0.45600 Val Loss:1.04996\n",
            "Epoch:2/6 Batch:200 Train Accuracy:0.48000 Train Loss:1.02725 Val Accuracy:0.43600 Val Loss:1.04978\n",
            "Epoch:2/6 Batch:210 Train Accuracy:0.44000 Train Loss:1.05402 Val Accuracy:0.42000 Val Loss:1.04938\n",
            "Epoch:2/6 Batch:220 Train Accuracy:0.39600 Train Loss:1.07153 Val Accuracy:0.40000 Val Loss:1.05531\n",
            "Epoch:2/6 Batch:230 Train Accuracy:0.42000 Train Loss:1.06441 Val Accuracy:0.42000 Val Loss:1.04946\n",
            "Epoch:2/6 Batch:240 Train Accuracy:0.37600 Train Loss:1.06511 Val Accuracy:0.40400 Val Loss:1.05203\n",
            "Epoch:2/6 Batch:250 Train Accuracy:0.38800 Train Loss:1.05267 Val Accuracy:0.32400 Val Loss:1.05182\n",
            "Epoch:2/6 Batch:260 Train Accuracy:0.45600 Train Loss:1.04128 Val Accuracy:0.43600 Val Loss:1.05070\n",
            "Epoch:2/6 Batch:270 Train Accuracy:0.46800 Train Loss:1.05502 Val Accuracy:0.40000 Val Loss:1.04972\n",
            "Epoch:2/6 Batch:280 Train Accuracy:0.40800 Train Loss:1.04674 Val Accuracy:0.42400 Val Loss:1.04959\n",
            "Epoch:3/6 Batch:290 Train Accuracy:0.39600 Train Loss:1.06296 Val Accuracy:0.45200 Val Loss:1.04950\n",
            "Epoch:3/6 Batch:300 Train Accuracy:0.46400 Train Loss:1.05197 Val Accuracy:0.44400 Val Loss:1.04934\n",
            "Epoch:3/6 Batch:310 Train Accuracy:0.45600 Train Loss:1.02812 Val Accuracy:0.44400 Val Loss:1.04944\n",
            "Epoch:3/6 Batch:320 Train Accuracy:0.40800 Train Loss:1.06386 Val Accuracy:0.40800 Val Loss:1.04971\n",
            "Epoch:3/6 Batch:330 Train Accuracy:0.47200 Train Loss:1.03674 Val Accuracy:0.40400 Val Loss:1.04914\n",
            "Epoch:3/6 Batch:340 Train Accuracy:0.40400 Train Loss:1.06644 Val Accuracy:0.40000 Val Loss:1.05018\n",
            "Epoch:3/6 Batch:350 Train Accuracy:0.42400 Train Loss:1.05101 Val Accuracy:0.39200 Val Loss:1.04922\n",
            "Epoch:3/6 Batch:360 Train Accuracy:0.42400 Train Loss:1.05935 Val Accuracy:0.37600 Val Loss:1.05173\n",
            "Epoch:3/6 Batch:370 Train Accuracy:0.49600 Train Loss:1.04424 Val Accuracy:0.44800 Val Loss:1.04899\n",
            "Epoch:3/6 Batch:380 Train Accuracy:0.46400 Train Loss:1.01825 Val Accuracy:0.41200 Val Loss:1.04922\n",
            "Epoch:3/6 Batch:390 Train Accuracy:0.36800 Train Loss:1.09240 Val Accuracy:0.43200 Val Loss:1.04883\n",
            "Epoch:3/6 Batch:400 Train Accuracy:0.40800 Train Loss:1.06258 Val Accuracy:0.39200 Val Loss:1.04805\n",
            "Epoch:3/6 Batch:410 Train Accuracy:0.49200 Train Loss:1.02339 Val Accuracy:0.36800 Val Loss:1.05033\n",
            "Epoch:3/6 Batch:420 Train Accuracy:0.50800 Train Loss:1.04826 Val Accuracy:0.46000 Val Loss:1.04753\n",
            "Epoch:3/6 Batch:430 Train Accuracy:0.50800 Train Loss:1.02899 Val Accuracy:0.42000 Val Loss:1.03759\n",
            "Epoch:4/6 Batch:440 Train Accuracy:0.54000 Train Loss:1.00747 Val Accuracy:0.56800 Val Loss:1.00003\n",
            "Epoch:4/6 Batch:450 Train Accuracy:0.57200 Train Loss:0.95712 Val Accuracy:0.46400 Val Loss:1.00116\n",
            "Epoch:4/6 Batch:460 Train Accuracy:0.55600 Train Loss:0.97837 Val Accuracy:0.55200 Val Loss:0.96671\n",
            "Epoch:4/6 Batch:470 Train Accuracy:0.55600 Train Loss:0.97010 Val Accuracy:0.54000 Val Loss:0.96735\n",
            "Epoch:4/6 Batch:480 Train Accuracy:0.58800 Train Loss:0.95427 Val Accuracy:0.60000 Val Loss:0.95700\n",
            "Epoch:4/6 Batch:490 Train Accuracy:0.57600 Train Loss:0.95880 Val Accuracy:0.52800 Val Loss:0.95499\n",
            "Epoch:4/6 Batch:500 Train Accuracy:0.57600 Train Loss:0.96482 Val Accuracy:0.58000 Val Loss:0.95507\n",
            "Epoch:4/6 Batch:510 Train Accuracy:0.53200 Train Loss:0.98989 Val Accuracy:0.56000 Val Loss:0.98168\n",
            "Epoch:4/6 Batch:520 Train Accuracy:0.56000 Train Loss:0.96552 Val Accuracy:0.57600 Val Loss:0.94790\n",
            "Epoch:4/6 Batch:530 Train Accuracy:0.60000 Train Loss:0.92097 Val Accuracy:0.50400 Val Loss:1.00005\n",
            "Epoch:4/6 Batch:540 Train Accuracy:0.60400 Train Loss:0.92799 Val Accuracy:0.58800 Val Loss:0.95085\n",
            "Epoch:4/6 Batch:550 Train Accuracy:0.62000 Train Loss:0.90902 Val Accuracy:0.49600 Val Loss:0.98624\n",
            "Epoch:4/6 Batch:560 Train Accuracy:0.54400 Train Loss:0.96066 Val Accuracy:0.52400 Val Loss:0.96353\n",
            "Epoch:4/6 Batch:570 Train Accuracy:0.60000 Train Loss:0.94801 Val Accuracy:0.60400 Val Loss:0.94270\n",
            "Epoch:5/6 Batch:580 Train Accuracy:0.60400 Train Loss:0.92739 Val Accuracy:0.55600 Val Loss:0.94913\n",
            "Epoch:5/6 Batch:590 Train Accuracy:0.57600 Train Loss:0.97841 Val Accuracy:0.56400 Val Loss:0.94124\n",
            "Epoch:5/6 Batch:600 Train Accuracy:0.56400 Train Loss:0.95334 Val Accuracy:0.58400 Val Loss:0.94803\n",
            "Epoch:5/6 Batch:610 Train Accuracy:0.64000 Train Loss:0.89561 Val Accuracy:0.58000 Val Loss:0.93573\n",
            "Epoch:5/6 Batch:620 Train Accuracy:0.61200 Train Loss:0.92918 Val Accuracy:0.54000 Val Loss:0.93909\n",
            "Epoch:5/6 Batch:630 Train Accuracy:0.56800 Train Loss:0.95752 Val Accuracy:0.58000 Val Loss:0.93452\n",
            "Epoch:5/6 Batch:640 Train Accuracy:0.64400 Train Loss:0.91011 Val Accuracy:0.58400 Val Loss:0.93824\n",
            "Epoch:5/6 Batch:650 Train Accuracy:0.60800 Train Loss:0.91930 Val Accuracy:0.56400 Val Loss:0.93528\n",
            "Epoch:5/6 Batch:660 Train Accuracy:0.63600 Train Loss:0.89954 Val Accuracy:0.60000 Val Loss:0.93316\n",
            "Epoch:5/6 Batch:670 Train Accuracy:0.57600 Train Loss:0.95488 Val Accuracy:0.57200 Val Loss:0.93887\n",
            "Epoch:5/6 Batch:680 Train Accuracy:0.58400 Train Loss:0.94941 Val Accuracy:0.56800 Val Loss:0.94390\n",
            "Epoch:5/6 Batch:690 Train Accuracy:0.61200 Train Loss:0.91887 Val Accuracy:0.55200 Val Loss:0.93445\n",
            "Epoch:5/6 Batch:700 Train Accuracy:0.62000 Train Loss:0.91011 Val Accuracy:0.58400 Val Loss:0.92398\n",
            "Epoch:5/6 Batch:710 Train Accuracy:0.57600 Train Loss:0.92201 Val Accuracy:0.61200 Val Loss:0.92118\n",
            "Epoch:5/6 Batch:720 Train Accuracy:0.49600 Train Loss:0.93773 Val Accuracy:0.55200 Val Loss:0.92110\n",
            "Epoch:6/6 Batch:730 Train Accuracy:0.65200 Train Loss:0.88893 Val Accuracy:0.62000 Val Loss:0.92298\n",
            "Epoch:6/6 Batch:740 Train Accuracy:0.66800 Train Loss:0.87760 Val Accuracy:0.60400 Val Loss:0.91971\n",
            "Epoch:6/6 Batch:750 Train Accuracy:0.65600 Train Loss:0.86899 Val Accuracy:0.58000 Val Loss:0.92237\n",
            "Epoch:6/6 Batch:760 Train Accuracy:0.62000 Train Loss:0.90140 Val Accuracy:0.56400 Val Loss:0.92804\n",
            "Epoch:6/6 Batch:770 Train Accuracy:0.62800 Train Loss:0.90419 Val Accuracy:0.60400 Val Loss:0.91428\n",
            "Epoch:6/6 Batch:780 Train Accuracy:0.58400 Train Loss:0.92788 Val Accuracy:0.60400 Val Loss:0.92748\n",
            "Epoch:6/6 Batch:790 Train Accuracy:0.58800 Train Loss:0.92560 Val Accuracy:0.48400 Val Loss:0.92646\n",
            "Epoch:6/6 Batch:800 Train Accuracy:0.60000 Train Loss:0.91967 Val Accuracy:0.60800 Val Loss:0.91409\n",
            "Epoch:6/6 Batch:810 Train Accuracy:0.64000 Train Loss:0.90477 Val Accuracy:0.58400 Val Loss:0.91832\n",
            "Epoch:6/6 Batch:820 Train Accuracy:0.59200 Train Loss:0.93087 Val Accuracy:0.56000 Val Loss:0.91721\n",
            "Epoch:6/6 Batch:830 Train Accuracy:0.63200 Train Loss:0.89288 Val Accuracy:0.57600 Val Loss:0.91378\n",
            "Epoch:6/6 Batch:840 Train Accuracy:0.61200 Train Loss:0.91495 Val Accuracy:0.58800 Val Loss:0.92598\n",
            "Epoch:6/6 Batch:850 Train Accuracy:0.59600 Train Loss:0.91439 Val Accuracy:0.58400 Val Loss:0.91488\n",
            "Epoch:6/6 Batch:860 Train Accuracy:0.60800 Train Loss:0.92175 Val Accuracy:0.60000 Val Loss:0.92138\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}